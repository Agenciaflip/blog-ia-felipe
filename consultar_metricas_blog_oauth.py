#!/usr/bin/env python3
"""
Consulta mÃ©tricas de trÃ¡fego do blog no Google Search Console.
"""
import json
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from datetime import datetime, timedelta

# Credentials OAuth
TOKEN_FILE = '/Users/felipezanonimini/Desktop/automacoes/credentials/search_console_token.json'
SITE_URL = 'sc-domain:agenciacafeonline.com.br'

def consultar_metricas():
    """Consulta mÃ©tricas dos Ãºltimos 7 dias."""
    print("\nðŸ“Š Consultando mÃ©tricas Google Search Console...")
    print(f"Property: {SITE_URL}\n")

    # Carregar token OAuth
    with open(TOKEN_FILE, 'r') as f:
        token_data = json.load(f)

    credentials = Credentials(
        token=token_data['token'],
        refresh_token=token_data['refresh_token'],
        token_uri=token_data['token_uri'],
        client_id=token_data['client_id'],
        client_secret=token_data['client_secret'],
        scopes=token_data['scopes']
    )

    service = build('searchconsole', 'v1', credentials=credentials)

    # PerÃ­odo: Ãºltimos 7 dias
    end_date = datetime.now().date()
    start_date = end_date - timedelta(days=7)

    # Query para mÃ©tricas gerais
    request = {
        'startDate': str(start_date),
        'endDate': str(end_date),
        'dimensions': ['page'],
        'rowLimit': 25
    }

    try:
        response = service.searchanalytics().query(
            siteUrl=SITE_URL, body=request
        ).execute()

        if 'rows' not in response:
            print("âš ï¸ Sem dados de trÃ¡fego nos Ãºltimos 7 dias\n")
            print("PossÃ­veis motivos:")
            print("  â€¢ Blog muito novo (precisa 2-4 semanas para aparecer)")
            print("  â€¢ Artigos ainda nÃ£o indexados pelo Google")
            print("  â€¢ Nenhum clique/impressÃ£o neste perÃ­odo")
            return

        # Calcular totais
        total_clicks = sum(row.get('clicks', 0) for row in response['rows'])
        total_impressions = sum(row.get('impressions', 0) for row in response['rows'])

        print(f"ðŸ“… PerÃ­odo: {start_date} atÃ© {end_date} (Ãºltimos 7 dias)\n")
        print(f"ðŸŽ¯ TOTAIS:")
        print(f"  â€¢ Cliques: {total_clicks}")
        print(f"  â€¢ ImpressÃµes: {total_impressions}")

        if total_impressions > 0:
            ctr = (total_clicks / total_impressions) * 100
            print(f"  â€¢ CTR: {ctr:.2f}%")

        print(f"\nðŸ“„ Top pÃ¡ginas com trÃ¡fego:")
        print("-" * 90)

        for i, row in enumerate(response['rows'][:15], 1):
            url = row['keys'][0]
            clicks = row.get('clicks', 0)
            impressions = row.get('impressions', 0)
            ctr = row.get('ctr', 0) * 100
            position = row.get('position', 0)

            # Extrair slug do artigo
            if '/blog/' in url:
                slug = url.split('/blog/')[-1].rstrip('/')
            else:
                slug = 'home' if url.endswith('.com.br/') else url.split('/')[-1]

            print(f"{i:2d}. {slug[:65]:<65}")
            print(f"    Cliques: {clicks:3d} | ImpressÃµes: {impressions:4d} | CTR: {ctr:5.1f}% | Pos: {position:4.1f}")
            print()

    except Exception as e:
        print(f"âŒ Erro ao consultar: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    consultar_metricas()
